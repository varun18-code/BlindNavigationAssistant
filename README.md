# ğŸ¦¯ Blind Navigation Assistant

A real-time object detection and voice-guided navigation system to assist visually impaired users, built using Python, OpenCV, and Pyttsx3.

---

## ğŸ” Overview

The Blind Navigation Assistant is a vision-based application that detects everyday objects from a webcam feed using a pre-trained deep learning model and provides **directional voice feedback** to help visually impaired users navigate safely.

The system processes video frames, identifies relevant objects using **MobileNet SSD**, and uses **text-to-speech** to announce both the object and its spatial location (left, center, right). It also includes **collision warnings**, **night mode**, **smart filtering**, and automatic **snapshot logging**.

---

## ğŸ¯ Features

âœ… Real-time object detection via webcam  
âœ… Direction-aware voice alerts using Pyttsx3  
âœ… Collision warning for nearby obstacles  
âœ… Smart object filtering (ignores non-navigational objects)  
âœ… Night mode: Boosts brightness in low-light conditions  
âœ… Snapshot saving of close objects  
âœ… Auto-generated activity logs  
âœ… Lightweight and beginner-friendly codebase

---

## ğŸ“ Project Structure

BlindNavigationAssistant/
â”œâ”€â”€ models/                            # Pre-trained model files
â”‚   â”œâ”€â”€ MobileNetSSD_deploy.caffemodel
â”‚   â””â”€â”€ MobileNetSSD_deploy.prototxt
â”œâ”€â”€ snapshots/                         # Directory for saved snapshots
â”œâ”€â”€ main.py                            # Main application script
â”œâ”€â”€ detection_log.txt                  # Log file for detected objects
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # Project documentation
â””â”€â”€ LICENSE                            # Project license

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/BlindNavigationAssistant.git
cd BlindNavigationAssistant
2. Install Dependencies
Create a virtual environment (recommended), then install:

bash

pip install -r requirements.txt

3. Download the Model Files
Place the following in the models/ folder:

MobileNetSSD_deploy.prototxt

MobileNetSSD_deploy.caffemodel

4. Run the Application
bash
python main.py

ğŸ§  How It Works
The video feed is split into left, center, and right zones.

Detected objects are mapped to their position and announced (e.g., "Person ahead on the left").

If an object is too close, a collision alert is issued, a snapshot is saved, and the event is logged.

In low-light conditions, the system boosts brightness (Night Mode).

ğŸ“ˆ Future Enhancements
âœ… GUI with Tkinter or PyQt

âœ… Raspberry Pi deployment for wearable prototypes

âœ… Android app using Kivy or Flutter + TFLite

âœ… YOLOv8 upgrade for faster & more accurate detection

âœ… Distance estimation using stereo vision or ultrasonic sensors

ğŸ§ª Requirements
Python 3.7+

OpenCV

Numpy

Pyttsx3

Install with:

bash
pip install opencv-python pyttsx3 numpy

ğŸ“ License
This project is licensed under the MIT License.

ğŸ™Œ Acknowledgements
MobileNet SSD

OpenCV team

Python & Pyttsx3 community

ğŸ‘¤ Author
Varun Pingale
